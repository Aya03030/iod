{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYvQOebqLcfM"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JWvLBewLcfP"
   },
   "source": [
    "# Lab 8.4: Sentiment Analysis\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbhmKC6NLcfS"
   },
   "source": [
    "Based on the video tutorial **Text Classification with Machine Learning,SpaCy and Scikit(Sentiment Analysis)** by **Jesse E. Agbe (JCharis)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnuAMgbhLcfV"
   },
   "source": [
    "## Data Source: UCI\n",
    "### UCI - Machine Learning Repository\n",
    "- Center for Machine Learning and Intelligent Systems\n",
    "\n",
    "The [**UCI Machine Learning Repository**](http://archive.ics.uci.edu/ml/about.html) is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms.\n",
    "\n",
    "### Dataset\n",
    "- [Sentiment Labelled Sentences Data Set](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n",
    "\n",
    "### Abstract\n",
    "The dataset contains sentences labelled with positive or negative sentiment.\n",
    "\n",
    "- Data Set Characteristics: Text\n",
    "- Number of Instances: 3000\n",
    "- Area: N/A\n",
    "- Attribute Characteristics: N/A\n",
    "- Number of Attributes: N/A\n",
    "- Date Donated: 2015-05-30\n",
    "- Associated Tasks: Classification\n",
    "- Missing Values? N/A\n",
    "- Number of Web Hits: 102584\n",
    "\n",
    "### Source\n",
    "Dimitrios Kotzias dkotzias '@' ics.uci.edu\n",
    "\n",
    "### Data Set Information\n",
    "This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n",
    "\n",
    "Please cite the paper if you want to use it :)\n",
    "\n",
    "It contains sentences labelled with positive or negative sentiment.\n",
    "\n",
    "### Format\n",
    "sentence &lt;tab&gt; score &lt;newline&gt;\n",
    "\n",
    "### Details\n",
    "Score is either 1 (for positive) or 0 (for negative)\n",
    "\n",
    "The sentences come from three different websites/fields:\n",
    "- imdb.com\n",
    "- amazon.com\n",
    "- yelp.com\n",
    "\n",
    "For each website, there exist **500 positive** and **500 negative** sentences. Those were selected randomly for larger datasets of reviews.\n",
    "\n",
    "We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.\n",
    "\n",
    "For the full datasets look:\n",
    "\n",
    "- **imdb**: Maas et. al., 2011 _Learning word vectors for sentiment analysis_\n",
    "- **amazon**: McAuley et. al., 2013 _Hidden factors and hidden topics: Understanding rating dimensions with review text_\n",
    "- **yelp**: [Yelp dataset challenge](http://www.yelp.com/dataset_challenge)\n",
    "\n",
    "\n",
    "### Attribute Information\n",
    "The attributes are text sentences, extracted from reviews of products, movies, and restaurants\n",
    "\n",
    "### Relevant Papers\n",
    "**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015\n",
    "\n",
    "### Citation Request\n",
    "**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abNvVWdlLcfW"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:27:26.865620Z",
     "start_time": "2019-06-17T01:27:24.368522Z"
    },
    "id": "4BJWjM0zLcfZ"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import pandas as pd\n",
    "\n",
    "import regex as re\n",
    "import spacy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzzk6JdcLcfh"
   },
   "source": [
    "## Load data\n",
    "\n",
    "Load Yelp, Amazon and Imdb Data.\n",
    "\n",
    "Hint: Source is separated by <tab>s and has no headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:29:38.157718Z",
     "start_time": "2019-06-17T01:29:38.152747Z"
    },
    "id": "GZUWhcCuLcfi"
   },
   "outputs": [],
   "source": [
    "yelp_text = 'yelp_labelled.txt'\n",
    "imdb_text = 'imdb_labelled_fixed.txt'\n",
    "amazon_text = 'amazon_cells_labelled.txt'\n",
    "\n",
    "# ANSWER\n",
    "\n",
    "df_yelp = pd.read_csv('/Users/ayano/Desktop/Data Science & AI/csv/yelp_labelled.txt', header = None, sep = '\\t')\n",
    "df_imdb = pd.read_csv('/Users/ayano/Desktop/Data Science & AI/csv/imdb_labelled_fixed.txt', header = None, sep = '\\t')\n",
    "df_amazon = pd.read_csv('/Users/ayano/Desktop/Data Science & AI/csv/amazon_cells_labelled.txt', header = None, sep = '\\t')\n",
    "dfs = {'yelp': df_yelp, 'imdb': df_imdb, 'amazon': df_amazon}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwa3MBrwLcfo"
   },
   "source": [
    "## Inspect the data\n",
    "\n",
    "Check your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:30:01.495935Z",
     "start_time": "2019-06-17T01:30:01.492941Z"
    },
    "id": "NddGh-EQLcfq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: yelp [1000 row(s) x 2 col(s)]\n",
      "                                                   0  1\n",
      "0                           Wow... Loved this place.  1\n",
      "1                                 Crust is not good.  0\n",
      "2          Not tasty and the texture was just nasty.  0\n",
      "3  Stopped by during the late May bank holiday of...  1\n",
      "4  The selection on the menu was great and so wer...  1\n",
      "\n",
      "Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1000 non-null   object\n",
      " 1   1       1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "---------------------------------------------------------------------------\n",
      "Dataset: imdb [1000 row(s) x 2 col(s)]\n",
      "                                                   0  1\n",
      "0  A very, very, very slow-moving, aimless movie ...  0\n",
      "1  Not sure who was more lost - the flat characte...  0\n",
      "2  Attempting artiness with black & white and cle...  0\n",
      "3       Very little music or anything to speak of.    0\n",
      "4  The best scene in the movie was when Gerardo i...  1\n",
      "\n",
      "Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1000 non-null   object\n",
      " 1   1       1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "---------------------------------------------------------------------------\n",
      "Dataset: amazon [1000 row(s) x 2 col(s)]\n",
      "                                                   0  1\n",
      "0  So there is no way for me to plug it in here i...  0\n",
      "1                        Good case, Excellent value.  1\n",
      "2                             Great for the jawbone.  1\n",
      "3  Tied to charger for conversations lasting more...  0\n",
      "4                                  The mic is great.  1\n",
      "\n",
      "Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1000 non-null   object\n",
      " 1   1       1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# データを辞書にまとめる\n",
    "dfs = {'yelp': df_yelp, 'imdb': df_imdb, 'amazon': df_amazon}\n",
    "\n",
    "# Inspect the Data\n",
    "for ds, df in dfs.items():\n",
    "    print(f'Dataset: {ds} [{df.shape[0]} row(s) x {df.shape[1]} col(s)]')\n",
    "    print(df.head())  # 最初の数行を表示\n",
    "    print('\\nSummary:')\n",
    "    print(df.info())  # データの概要を表示\n",
    "    print('-' * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meEtfGfELcf4"
   },
   "source": [
    "## Merge the data\n",
    "\n",
    "Merge all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:30:37.302897Z",
     "start_time": "2019-06-17T01:30:37.299903Z"
    },
    "id": "WVpAx-HHcbwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset: [3000 row(s) x 2 col(s)]\n",
      "                                                text  sentiment\n",
      "0                           Wow... Loved this place.          1\n",
      "1                                 Crust is not good.          0\n",
      "2          Not tasty and the texture was just nasty.          0\n",
      "3  Stopped by during the late May bank holiday of...          1\n",
      "4  The selection on the menu was great and so wer...          1\n",
      "\n",
      "Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       3000 non-null   object\n",
      " 1   sentiment  3000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n",
      "None\n",
      "\n",
      "Label Distribution:\n",
      "1    1500\n",
      "0    1500\n",
      "Name: sentiment, dtype: int64\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# 列名を割り当て\n",
    "df_yelp.columns = ['text', 'sentiment']\n",
    "df_imdb.columns = ['text', 'sentiment']\n",
    "df_amazon.columns = ['text', 'sentiment']\n",
    "\n",
    "# データを結合\n",
    "merged_df = pd.concat([df_yelp, df_imdb, df_amazon], ignore_index=True)\n",
    "\n",
    "# Inspect the Merged Data\n",
    "print(f'Merged Dataset: [{merged_df.shape[0]} row(s) x {merged_df.shape[1]} col(s)]')\n",
    "print(merged_df.head())  # 最初の数行を表示\n",
    "print('\\nSummary:')\n",
    "print(merged_df.info())  # データの概要を表示\n",
    "print('\\nLabel Distribution:')\n",
    "print(merged_df['sentiment'].value_counts())  # 感情ラベルの分布を表示\n",
    "print('-' * 75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBIFtbMALcf8"
   },
   "source": [
    "## Export the data\n",
    "\n",
    "Export merged datasets to as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:31:16.041727Z",
     "start_time": "2019-06-17T01:31:16.037738Z"
    },
    "id": "n8OLkaALLcf9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "# CSVファイルにデータをエクスポート\n",
    "merged_df.to_csv('/Users/ayano/Desktop/Data Science & AI/csv/merged_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzA4FQsPLcgA"
   },
   "source": [
    "## Prepare the stage\n",
    "- Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:31:19.686599Z",
     "start_time": "2019-06-17T01:31:18.952239Z"
    },
    "id": "wVMTSDYQLcgB"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YguMrtDuLcgD"
   },
   "source": [
    "## Prepare the text\n",
    "All the text handling and preparation concerned with the changes and modifications from the raw source text to a format that will be used for the actual processing, things like:\n",
    "- handle encoding\n",
    "- handle extraneous and international characters\n",
    "- handle symbols\n",
    "- handle metadata and embedded information\n",
    "- handle repetitions (such multiple spaces or newlines)\n",
    "\n",
    "Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:31:31.608285Z",
     "start_time": "2019-06-17T01:31:31.601306Z"
    },
    "id": "GlsKSvonLcgD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # reduce multiple spaces and newlines to only one\n",
    "    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n",
    "    # remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:32:56.768268Z",
     "start_time": "2019-06-17T01:32:56.765283Z"
    },
    "id": "upPa3YmmLcgF"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za_6vt3MLcgH"
   },
   "source": [
    "## Work the text\n",
    "Concern with the meaning and the substance of the content to extract actual information.\n",
    "\n",
    "Hint: Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:32:58.911623Z",
     "start_time": "2019-06-17T01:32:58.897659Z"
    },
    "id": "sh_uDWcCLcgI"
   },
   "outputs": [],
   "source": [
    "def convert_text(text):\n",
    "    '''\n",
    "    Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc.\n",
    "    '''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:33:42.014624Z",
     "start_time": "2019-06-17T01:33:01.620538Z"
    },
    "id": "0vDv55U1LcgK",
    "outputId": "6ae31463-3509-4ea6-934a-6ea1ff1f6b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 ms, sys: 6.62 ms, total: 9.29 ms\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['short'] = df['text'].apply(convert_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:13.381487Z",
     "start_time": "2019-06-17T01:35:13.362526Z"
    },
    "id": "faiuJfunLcgM",
    "outputId": "67d67ec3-44c6-4315-ef42-7467f69494d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Pretty piece of junk.</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretty piece of junk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>During several different 2 minute calls, I exp...</td>\n",
       "      <td>0</td>\n",
       "      <td>During several different 2 minute calls, I exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>Too bad you have to pay up to $$$ a month for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Too bad you have to pay up to $$$ a month for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>I got the aluminum case for my new Palm VX and...</td>\n",
       "      <td>1</td>\n",
       "      <td>I got the aluminum case for my new Palm VX and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Doesn't hold charge.</td>\n",
       "      <td>0</td>\n",
       "      <td>Doesn't hold charge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>I highly recommend this modest priced cellular...</td>\n",
       "      <td>1</td>\n",
       "      <td>I highly recommend this modest priced cellular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Good product, good seller.</td>\n",
       "      <td>1</td>\n",
       "      <td>Good product, good seller.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>The best phone in market :).</td>\n",
       "      <td>1</td>\n",
       "      <td>The best phone in market :).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>very good product, well made.</td>\n",
       "      <td>1</td>\n",
       "      <td>very good product, well made.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>After trying many many handsfree gadgets this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>After trying many many handsfree gadgets this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment  \\\n",
       "494                              Pretty piece of junk.          0   \n",
       "141  During several different 2 minute calls, I exp...          0   \n",
       "688  Too bad you have to pay up to $$$ a month for ...          0   \n",
       "513  I got the aluminum case for my new Palm VX and...          1   \n",
       "29                                Doesn't hold charge.          0   \n",
       "433  I highly recommend this modest priced cellular...          1   \n",
       "280                         Good product, good seller.          1   \n",
       "387                       The best phone in market :).          1   \n",
       "514                      very good product, well made.          1   \n",
       "383  After trying many many handsfree gadgets this ...          1   \n",
       "\n",
       "                                                 short  \n",
       "494                              Pretty piece of junk.  \n",
       "141  During several different 2 minute calls, I exp...  \n",
       "688  Too bad you have to pay up to $$$ a month for ...  \n",
       "513  I got the aluminum case for my new Palm VX and...  \n",
       "29                                Doesn't hold charge.  \n",
       "433  I highly recommend this modest priced cellular...  \n",
       "280                         Good product, good seller.  \n",
       "387                       The best phone in market :).  \n",
       "514                      very good product, well made.  \n",
       "383  After trying many many handsfree gadgets this ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbwjijVyLcgP"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:22.212854Z",
     "start_time": "2019-06-17T01:35:22.040284Z"
    },
    "id": "eJZpD903LcgQ"
   },
   "outputs": [],
   "source": [
    "# helper function to show results and charts\n",
    "def show_summary_report(actual, prediction, probabilities):\n",
    "\n",
    "    if isinstance(actual, pd.Series):\n",
    "        actual = actual.values.astype(int)\n",
    "    prediction = prediction.astype(int)\n",
    "\n",
    "    accuracy_ = accuracy_score(actual, prediction)\n",
    "    precision_ = precision_score(actual, prediction)\n",
    "    recall_ = recall_score(actual, prediction)\n",
    "    roc_auc_ = roc_auc_score(actual, probabilities)\n",
    "\n",
    "    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n",
    "    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n",
    "    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n",
    "    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n",
    "    print('-' * 107)\n",
    "    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    mat = confusion_matrix(actual, prediction)\n",
    "\n",
    "    # Precision/Recall\n",
    "    precision, recall, _ = precision_recall_curve(actual, probabilities)\n",
    "    average_precision = average_precision_score(actual, probabilities)\n",
    "    \n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(actual, probabilities)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n",
    "\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "    ax[0].set_xlabel('True label')\n",
    "    ax[0].set_ylabel('Predicted label')\n",
    "    \n",
    "    # Precision/Recall\n",
    "    step_kwargs = {'step': 'post'}\n",
    "    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
    "    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n",
    "    ax[1].set_ylim([0.0, 1.0])\n",
    "    ax[1].set_xlim([0.0, 1.0])\n",
    "    ax[1].set_xlabel('Recall')\n",
    "    ax[1].set_ylabel('Precision')\n",
    "    ax[1].set_title('2-class Precision-Recall curve')\n",
    "\n",
    "    # ROC\n",
    "    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n",
    "    ax[2].set_xlim([0.0, 1.0])\n",
    "    ax[2].set_ylim([0.0, 1.0])\n",
    "    ax[2].set_xlabel('False Positive Rate')\n",
    "    ax[2].set_ylabel('True Positive Rate')\n",
    "    ax[2].set_title('Receiver Operating Characteristic')\n",
    "    ax[2].legend(loc = 'lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return (accuracy_, precision_, recall_, roc_auc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:24.658233Z",
     "start_time": "2019-06-17T01:35:24.649227Z"
    },
    "id": "Hj2aoBqqLcgV"
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = df['short']\n",
    "y = df['sentiment']\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr_VmeNMLcgY"
   },
   "source": [
    "## Use Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:32.373670Z",
     "start_time": "2019-06-17T01:35:32.369681Z"
    },
    "id": "Rhd__LD6LcgZ"
   },
   "outputs": [],
   "source": [
    "# create a matrix of word counts from the text\n",
    "counts = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:35.842101Z",
     "start_time": "2019-06-17T01:35:35.784219Z"
    },
    "id": "23CpVgPxLcgb"
   },
   "outputs": [],
   "source": [
    "# do the actual counting\n",
    "A = counts.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:38.590493Z",
     "start_time": "2019-06-17T01:35:38.586469Z"
    },
    "id": "c_rue57RLcgd"
   },
   "outputs": [],
   "source": [
    "# create a classifier using SVC\n",
    "classifier = SVC(kernel='linear', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:41.929126Z",
     "start_time": "2019-06-17T01:35:41.745617Z"
    },
    "id": "Lou4xDLmLcgh",
    "outputId": "cbc743d3-09c6-448c-bd75-75f9d5366211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the classifier with the training data\n",
    "classifier.fit(A.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:47.210207Z",
     "start_time": "2019-06-17T01:35:47.199250Z"
    },
    "id": "inkg1KTiLcgi"
   },
   "outputs": [],
   "source": [
    "# do the transformation for the test data\n",
    "# NOTE: use `transform()` instead of `fit_transform()`\n",
    "B = counts.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:51.223067Z",
     "start_time": "2019-06-17T01:35:51.209096Z"
    },
    "id": "dg-HpdJ0Lcgk",
    "outputId": "e85d967d-1aa1-41bf-c0de-04a0c4687221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions based on the test data\n",
    "predictions = classifier.predict(B.toarray())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:35:54.779047Z",
     "start_time": "2019-06-17T01:35:54.771069Z"
    },
    "id": "t0HJn9qhLcgm",
    "outputId": "0bc7328f-ed1e-4259-e02f-981413ab8bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Ia6a8ULcgn"
   },
   "source": [
    "## Repeat using TF-IDF\n",
    "TF-IDF = Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:36:02.927008Z",
     "start_time": "2019-06-17T01:36:02.785387Z"
    },
    "id": "7Tg1dwSpLcgo",
    "outputId": "256d6cbb-663b-4f6d-daa6-c609c9ec18ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "# create a matrix of word counts from the text\n",
    "# use TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "# do the actual counting\n",
    "A = tfidf.fit_transform(X_train, y_train)\n",
    "\n",
    "# train the classifier with the training data\n",
    "classifier.fit(A.toarray(), y_train)\n",
    "\n",
    "# do the transformation for the test data\n",
    "# NOTE: use `transform()` instead of `fit_transform()`\n",
    "B = tfidf.transform(X_test)\n",
    "\n",
    "# make predictions based on the test data\n",
    "predictions = classifier.predict(B.toarray())\n",
    "\n",
    "# check the accuracy\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5PTu402Lcgq"
   },
   "source": [
    "## Repeating it all for comparision\n",
    "Repeating the whole lot in one big block\n",
    "\n",
    "Find 'Accuracy', 'Precision', 'Recall', 'ROC_AUC' using CountVectorizer and TfidfVectorizer and keep the result in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:37:30.200048Z",
     "start_time": "2019-06-17T01:37:30.197044Z"
    },
    "id": "_98CzdfPLcgq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Vectorizer  Accuracy  Precision    Recall   ROC_AUC\n",
      "0  CountVectorizer     0.845   0.858491  0.850467  0.915285\n",
      "1  TfidfVectorizer     0.865   0.908163  0.831776  0.937795\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with 'text' and 'label' columns\n",
    "# Replace 'df' with your actual DataFrame if needed\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(vectorizer, X_train, X_test, y_train, y_test):\n",
    "    # Transform text data into features\n",
    "    A_train = vectorizer.fit_transform(X_train)\n",
    "    A_test = vectorizer.transform(X_test)\n",
    "\n",
    "    # Create a classifier using SVC\n",
    "    classifier = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    # Train the classifier with the training data\n",
    "    classifier.fit(A_train.toarray(), y_train)\n",
    "    \n",
    "    # Make predictions based on the test data\n",
    "    predictions = classifier.predict(A_test.toarray())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, classifier.predict_proba(A_test.toarray())[:, 1])\n",
    "\n",
    "    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'ROC_AUC': roc_auc}\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Vectorizer', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "# Train and evaluate with CountVectorizer\n",
    "count_vectorizer_results = train_and_evaluate(CountVectorizer(), X_train, X_test, y_train, y_test)\n",
    "results_df = results_df.append({'Vectorizer': 'CountVectorizer', **count_vectorizer_results}, ignore_index=True)\n",
    "\n",
    "# Train and evaluate with TfidfVectorizer\n",
    "tfidf_vectorizer_results = train_and_evaluate(TfidfVectorizer(), X_train, X_test, y_train, y_test)\n",
    "results_df = results_df.append({'Vectorizer': 'TfidfVectorizer', **tfidf_vectorizer_results}, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2023 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
